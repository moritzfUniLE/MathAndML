"""
Utility functions for NOTEARS causal discovery.
Common functions used across different algorithms and components.
"""
import numpy as np
import igraph as ig


def apply_threshold(W: np.ndarray, thresh: float | None) -> np.ndarray:
    """
    Apply threshold to adjacency matrix, zeroing small entries and diagonal.
    
    Args:
        W: Adjacency matrix
        thresh: Threshold value (None to skip thresholding)
        
    Returns:
        Thresholded adjacency matrix
    """
    W_thr = W.copy()
    if thresh is not None: 
        W_thr[np.abs(W_thr) < thresh] = 0.0
    np.fill_diagonal(W_thr, 0.0)
    return W_thr


def load_ground_truth_from_bif(bif_path: str) -> tuple[np.ndarray|None, list[str]|None, bool]:
    """
    Parse BIF file to extract ground-truth DAG and node names.
    
    Args:
        bif_path: Path to BIF file
        
    Returns:
        Tuple of (adjacency_matrix, node_names, success_flag)
    """
    import re
    try:
        with open(bif_path, 'r') as f:
            text = f.read()
        
        vars = re.findall(r'variable\s+(\w+)', text)
        if not vars: 
            raise ValueError("No variables found in BIF")
        
        n = len(vars)
        idx = {v: i for i, v in enumerate(vars)}
        W = np.zeros((n, n))
        
        # Extract parent-child relations
        for m in re.finditer(r'probability\s*\(\s*(\w+)\s*\|\s*([^\)]+)\)', text):
            child = m.group(1)
            parents = [p.strip() for p in m.group(2).split(',')]
            for p in parents:
                if p in idx:  # Safety check
                    W[idx[p], idx[child]] = 1
        
        print(f"[INFO] Ground Truth loaded: {n} nodes")
        return W, vars, True
        
    except Exception as e:
        print(f"[ERROR] BIF load failed: {e}")
        return None, None, False


def compute_metrics(W_learned: np.ndarray, W_true: np.ndarray, thresh: float | None = None) -> dict:
    """
    Compute performance metrics comparing learned and true adjacency matrices.
    
    Args:
        W_learned: Learned adjacency matrix
        W_true: Ground truth adjacency matrix  
        thresh: Threshold for learned matrix
        
    Returns:
        Dictionary with metrics (precision, recall, F1, etc.) and any error message
    """
    try:
        if W_learned.shape != W_true.shape:
            raise ValueError(f"Shape mismatch: learned {W_learned.shape} vs true {W_true.shape}")
        
        # Apply threshold and convert to binary
        Wl = (apply_threshold(W_learned, thresh) != 0).astype(int)
        Wt = (W_true != 0).astype(int)
        
        # Compute confusion matrix elements
        TP = int(np.sum((Wl == 1) & (Wt == 1)))
        FP = int(np.sum((Wl == 1) & (Wt == 0)))
        FN = int(np.sum((Wl == 0) & (Wt == 1)))
        
        # Compute metrics
        precision = TP / (TP + FP) if TP + FP > 0 else 0
        recall = TP / (TP + FN) if TP + FN > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0
        
        return {
            'hamming_distance': int(np.sum(Wl != Wt)),
            'true_positives': TP,
            'false_positives': FP,
            'false_negatives': FN,
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'error': None
        }
        
    except Exception as e:
        return {
            'hamming_distance': None,
            'true_positives': None,
            'false_positives': None,
            'false_negatives': None,
            'precision': None,
            'recall': None,
            'f1_score': None,
            'error': str(e)
        }


def sigmoid(x):
    """Sigmoid function for logistic SEM."""
    return 1 / (1 + np.exp(-x))


def adjacency_matrix_to_bif(W: np.ndarray, node_names: list[str] = None) -> str:
    """
    Convert adjacency matrix to BIF (Bayesian Interchange Format) string.
    
    Args:
        W: Weighted adjacency matrix where W[i,j] != 0 indicates edge i -> j
        node_names: List of node names (default: X1, X2, ..., Xd)
        
    Returns:
        BIF format string representing the DAG structure
    """
    d = W.shape[0]
    if node_names is None:
        node_names = [f"X{i+1}" for i in range(d)]
    
    if len(node_names) != d:
        raise ValueError(f"Number of node names ({len(node_names)}) must match matrix dimension ({d})")
    
    # Convert to binary adjacency matrix
    B = (W != 0).astype(int)
    
    bif_lines = []
    bif_lines.append("// Synthetic DAG generated by NOTEARS Causal Discovery Platform")
    bif_lines.append("")
    
    # Variable declarations
    for i, name in enumerate(node_names):
        bif_lines.append(f"variable {name} {{")
        bif_lines.append("  type discrete [ 2 ] { v0, v1 };")
        bif_lines.append("}")
        bif_lines.append("")
    
    # Probability tables
    for j, child in enumerate(node_names):
        # Find parents (nodes that have edges pointing to j)
        parents = [node_names[i] for i in range(d) if B[i, j] == 1]
        
        if not parents:
            # Root node - no parents
            bif_lines.append(f"probability ( {child} ) {{")
            bif_lines.append("  table 0.5, 0.5;")
            bif_lines.append("}")
        else:
            # Node with parents
            parents_str = ", ".join(parents)
            bif_lines.append(f"probability ( {child} | {parents_str} ) {{")
            
            # For simplicity, use uniform probability tables
            # In practice, these would be learned from data
            num_parent_configs = 2 ** len(parents)
            for config in range(num_parent_configs):
                bif_lines.append("  table 0.5, 0.5;")
            
            bif_lines.append("}")
        
        bif_lines.append("")
    
    return "\n".join(bif_lines)


def create_artifical_dataset(n, d, s0, graph_type, sem_type):
    """Create an artificial dataset with a random DAG and samples.

    Args:
        n (int): num of samples
        d (int): num of nodes
        s0 (int): expected num of edges
        graph_type (str): ER, SF, BP
        sem_type (str): gauss, exp, gumbel, uniform, logistic, poisson

    Returns:
        X (np.ndarray): [n, d] sample matrix
        W (np.ndarray): [d, d] weighted adj matrix of DAG
    """
    B = simulate_dag(d, s0, graph_type)
    W = simulate_parameter(B)
    X = simulate_linear_sem(W, n, sem_type)
    return X, W


def is_dag(W):
    G = ig.Graph.Weighted_Adjacency(W.tolist())
    return G.is_dag()

def simulate_dag(d, s0, graph_type):
    """Simulate random DAG with some expected number of edges.

    Args:
        d (int): num of nodes
        s0 (int): expected num of edges
        graph_type (str): ER, SF, BP

    Returns:
        B (np.ndarray): [d, d] binary adj matrix of DAG
    """
    def _random_permutation(M):
        # np.random.permutation permutes first axis only
        P = np.random.permutation(np.eye(M.shape[0]))
        return P.T @ M @ P

    def _random_acyclic_orientation(B_und):
        return np.tril(_random_permutation(B_und), k=-1)

    def _graph_to_adjmat(G):
        return np.array(G.get_adjacency().data)

    if graph_type == 'ER':
        # Erdos-Renyi
        G_und = ig.Graph.Erdos_Renyi(n=d, m=s0)
        B_und = _graph_to_adjmat(G_und)
        B = _random_acyclic_orientation(B_und)
    elif graph_type == 'SF':
        # Scale-free, Barabasi-Albert
        G = ig.Graph.Barabasi(n=d, m=int(round(s0 / d)), directed=True)
        B = _graph_to_adjmat(G)
    elif graph_type == 'BP':
        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)
        top = int(0.2 * d)
        G = ig.Graph.Random_Bipartite(top, d - top, m=s0, directed=True, neimode=ig.OUT)
        B = _graph_to_adjmat(G)
    else:
        raise ValueError('unknown graph type')
    B_perm = _random_permutation(B)
    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()
    return B_perm

def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):
    """Simulate SEM parameters for a DAG.

    Args:
        B (np.ndarray): [d, d] binary adj matrix of DAG
        w_ranges (tuple): disjoint weight ranges

    Returns:
        W (np.ndarray): [d, d] weighted adj matrix of DAG
    """
    W = np.zeros(B.shape)
    S = np.random.randint(len(w_ranges), size=B.shape)  # which range
    for i, (low, high) in enumerate(w_ranges):
        U = np.random.uniform(low=low, high=high, size=B.shape)
        W += B * (S == i) * U
    return W

def simulate_linear_sem(W, n, sem_type, noise_scale=None):
    """Simulate samples from linear SEM with specified type of noise.

    For uniform, noise z ~ uniform(-a, a), where a = noise_scale.

    Args:
        W (np.ndarray): [d, d] weighted adj matrix of DAG
        n (int): num of samples, n=inf mimics population risk
        sem_type (str): gauss, exp, gumbel, uniform, logistic, poisson
        noise_scale (np.ndarray): scale parameter of additive noise, default all ones

    Returns:
        X (np.ndarray): [n, d] sample matrix, [d, d] if n=inf
    """
    def _simulate_single_equation(X, w, scale):
        """X: [n, num of parents], w: [num of parents], x: [n]"""
        if sem_type == 'gauss':
            z = np.random.normal(scale=scale, size=n)
            x = X @ w + z
        elif sem_type == 'exp':
            z = np.random.exponential(scale=scale, size=n)
            x = X @ w + z
        elif sem_type == 'gumbel':
            z = np.random.gumbel(scale=scale, size=n)
            x = X @ w + z
        elif sem_type == 'uniform':
            z = np.random.uniform(low=-scale, high=scale, size=n)
            x = X @ w + z
        elif sem_type == 'logistic':
            x = np.random.binomial(1, sigmoid(X @ w)) * 1.0
        elif sem_type == 'poisson':
            x = np.random.poisson(np.exp(X @ w)) * 1.0
        else:
            raise ValueError('unknown sem type')
        return x

    d = W.shape[0]
    if noise_scale is None:
        scale_vec = np.ones(d)
    elif np.isscalar(noise_scale):
        scale_vec = noise_scale * np.ones(d)
    else:
        if len(noise_scale) != d:
            raise ValueError('noise scale must be a scalar or has length d')
        scale_vec = noise_scale
    if not is_dag(W):
        raise ValueError('W must be a DAG')
    if np.isinf(n):  # population risk for linear gauss SEM
        if sem_type == 'gauss':
            # make 1/d X'X = true cov
            X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)
            return X
        else:
            raise ValueError('population risk not available')
    # empirical risk
    G = ig.Graph.Weighted_Adjacency(W.tolist())
    ordered_vertices = G.topological_sorting()
    assert len(ordered_vertices) == d
    X = np.zeros([n, d])
    for j in ordered_vertices:
        parents = G.neighbors(j, mode=ig.IN)
        X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])
    return X